{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zajęcia 1.\n",
    "\n",
    "Zakres:\n",
    "* zapoznanie sie z 2 srodowiskami pracy (klaster Hadoop oraz środowisko dostepne na GCP.\n",
    "* operacje na rozproszonym i obiektowym systemie plików (HDFS i GCS)\n",
    "* wykorzystanie platformy Apache Spark do interaktywnych kwerend na danych\n",
    "\n",
    "Języki:\n",
    "* bash, Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krótka lista przydatnych poleceń bash: https://www.reddit.com/r/linux/comments/9rns12/some_linux_commands_cheatsheet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klaster Hadoop\n",
    "\n",
    "\n",
    "URL Jupyter na klastrze Instytutu Informatyki: https://zsibio.ii.pw.edu.pl/jupyter/\n",
    "\n",
    "### Architektura klastra\n",
    "* edge node: cdh00 (węzeł dostępowy)\n",
    "* HDFS:\n",
    "  * name node: cdh01\n",
    "  * data nodes: cdh02-cdh05\n",
    "* YARN\n",
    "  * resource manager: cdh01\n",
    "  * node managers: cdh02-cdh05\n",
    "* Dystrybucja Hadoop: Hortonworks\n",
    "* konsola administracyjna: http://cdh01:8080/\n",
    "\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "* Interaktywny notatnik dostepny poprzez przeglądarkę.\n",
    "* Służy do wpisywania poleceń w wybranych jezykach programowania oraz opis w tzw jezyku markdown.\n",
    "* Polecenia/opis wpisujemy do komórek. Komórki są wykonywane przez tzw. kernel (np Python w określonej wersji)\n",
    "* Działa w tryb edycji komórek + tryb wykonywania poleceń. (przejście poprzez ESC i ENTER)\n",
    "* Podstawowe skróty klawiaturowe: \n",
    "  * Esc/Enter\n",
    "  * strzalki\n",
    "  * ctrl-enter/shift-enter\n",
    "  * a/b/d\n",
    "  * y/m \n",
    "* Moze wykonywać polecenia powłoki (bash) !  %%bash\n",
    "* Notatnik zapisywany jest w formacie ipynb (IPythonNoteBook)\n",
    "* Kolejnosc uruchomienia komórek moze byc rozna.\n",
    "* Mozliwosc zatrzymania kernela i uruchomienia od nowa. \n",
    "\n",
    "Polecamy film: https://www.youtube.com/watch?v=HW29067qVWk\n",
    "\n",
    "\n",
    "\n",
    "### Security\n",
    "Klaster jest zabezpieczony przed niepowolanym dostepem.\n",
    "Zazwyczaj dostep do klastra to: zalogowanie poprzez ssh do wezla dostepowego a potem wykonywanie polecen. \n",
    "My logujemy sie do Jupytera, w tle pobierany jest ticket dostepowy do odpowiednich zasobów klastra. Korzystajac ze zmiennych srodowiskowych i plikow konfiguracyjnych Jupyter wie jak polaczyc sie z menedzerem zasobow oraz namenodem HDFS.\n",
    "\n",
    "Dostep do Jupyter jest publiczny i po zajeciach mozna z niego korzystac."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kopiowanie danych do swojego katalogu domowego\n",
    "\n",
    "W katalogu `/data/local/datascience/data/` znajduje sie plik który bedzie nam dzisiaj słuzyl do pracy. Należy go skopiować do swojego katalogu domowego. \n",
    "Dane pochodzą z https://gdac.broadinstitute.org/ i zawierają dane z badaniami nad mutacjami typu CNV w obrebie genu BRAC2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd                # przejdz do katalogu domowego\n",
    "mkdir -p data     # stworz katalog data (i posrednie)\n",
    "cd data           # przejdz do katalogu data \n",
    "cp /data/local/datascience/data/brca.txt .  # skopiuj plik z lokalizacji do bieżącej lokalizacji\n",
    "ls -lah           # wylistuj zawartość bieżącego katalogu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head ~/data/brca.txt   # pokaż początek pliku\n",
    "echo                   # pusta linia\n",
    "tail ~/data/brca.txt   # pokaż koniec pliku \n",
    "echo\n",
    "wc -l ~/data/brca.txt  # policz linie pliku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Do tej pory korzystalismy z lokalnego systemu plików do ktorego ma dostep maszyna na ktora sie zalogowalismy (cdh00). Teraz zaczniemy korzystac z rozproszonego systemu plikow.\n",
    "\n",
    "## HDFS\n",
    "\n",
    "* Dane są rozproszone na wielu serwerach i dzielone na bloki o ustalonym rozmiarze. \n",
    "* Bloki zostają rozdystrybuowane pomiędzy węzłami. Mogą być replikowane między węzłami.\n",
    "* Name Node - odpowiedzialny za koordynację, zarządzanie metadanymi plików i aktywny monitoring replikacji plików.\n",
    "* Data Node - węzeł składujący zawartość plików.\n",
    "* Dostęp do danych, poprzez interfejs commandline, lub API programistyczne\n",
    "\n",
    "Zakres na dzisiejszym lab:\n",
    "* zapis i odczyt danych z HDFS\n",
    "* manipulacje na plikach i podstawowe komendy\n",
    "\n",
    "Dla podstawowych polecen systemowych na plikach istnieja odpowiedniki polecen dla systemu HDFS\n",
    "* `ls` -> `hdfs dfs -ls`\n",
    "* `cp` -> `hdfs dfs -cp`\n",
    "* `mv` -> `hdfs dfs -mv`\n",
    "* `rm` -> `hdfs dfs -rm`\n",
    "\n",
    "Pełna lista poleceń znajduje się na www: https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "### Listowanie swojego HOME na HDFS\n",
    "Każdy ma swoj katalog domowy na HDFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls    # wylistuj zawartość katalogu domowego (HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swoj katalog mozna tez sprawdzic podajac sciezke bezwgledna od katalogu glownego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /user/${USER} # wylistuj zawartość katalogu domowego (HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzmy katalog glowny na HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls / # wylistuj zawartość katalogu głównego (HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **** ZADANIE 1 ****\n",
    "Korzystajac z analogii do polecen znanych z lokalnego sytemu pliku oraz z dokumentacji polecen dla HDFS stworz w swoim katalogu domowym na HDFS katalog external a w nim katalog data. Sprawdz czy foldery istnieją."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dodanie pliku na HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -put ~/data/brca.txt /user/${USER}/external/data  # dodaj plik do wskazanej lokalizacji na HDFS\n",
    "hdfs dfs -ls /user/${USER}/external/data                   # wylistuj zawartość katalogu na HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaki jest rozmiar pliku? Mozna skorzystac z przelacznika -h w ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -h /user/${USER}/external/data   # rozmiar pliku "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A calkowita wielkosc pliku? Polecenie `du`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -du -h /user/${USER}/external/data/brca.txt  # całkowita przestrzeń zajmowana przez plik (replikacja)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informacje o statusie pliku: polecenie `fsck`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs fsck /user/${USER}/external/data/brca.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plik mozna rowniez odczytac bezposrednio z HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -head /user/${USER}/external/data/brca.txt # odczytaj początek pliku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **** ZADANIE 2 ****\n",
    "W swoim katalogu domowym stworz katalog external/temp. Skopiuj do niego plik brca.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usuwanie i odzyskiwanie plikow\n",
    "\n",
    "Do usuwanie sluzy polecenie -rm. Domyslnie usuwany plik jest przesuwany do kosza, skad mozna go odzyskac. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm /user/${USER}/external/temp/brca.txt # usun plik \n",
    "hdfs dfs -ls /user/${USER}/external/temp/          # wylistuj zawartość katalogu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dostalismy lokalizacje pliku z kosza, stamtad mozemy go skopiowac."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do samodzielnej weryfikacji:\n",
    "* Przywrócenie pliku z kosza\n",
    "* Zachowanie polecenia rm przy podaniu parametru `skipTrash`.\n",
    "* modyfikacja uprawnień na katalogu poprzez polecenie `chmod`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pobranie pliku z powrotem na lokalny dysk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get /user/${USER}/external/data/brca.txt brca2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "\n",
    "* Apache Spark platforma ogólnego zastosowania, opensource, do przetwarzania duzych zbiorow danych.\n",
    "* Posiada  API dla języków programowania: Scala, Python i R. \n",
    "* Przetwarzanie w Spark przetwarzanie jest wykonywane w większości  wprost w pamięci operacyjnej.\n",
    "* Przeznaczenie: do uruchamiania  aplikacji i skryptów z wykorzystaniem uczenia maszynowego lub interaktywnych kwerend.\n",
    "* Spark ten wspiera SQL (typ DataFrames), przetwarzanie strumieniowe oraz przetwarzanie grafów.\n",
    "* Integracja z lokalną pamięci masową, rozproszonymi lub obiektowymi systemu plików.\n",
    "* Spark można uruchamić na pojedynczej maszynie na środowisku klastrowym, lub w chmurze. \n",
    "\n",
    "Zakres na dzisiejsze laboratorium:\n",
    "* stworzenie sesji Spark\n",
    "* zaczytanie danych z pliku tekstowego\n",
    "* kwerendy na danych\n",
    "\n",
    "Na tych cwiczeniach bedziemy korzystac z pliku ktory zapisalismy na HDFS w pierwszej czesci cwicczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /user/${USER}/external/data # Sprawdzmy czy plik znajduje się w katalogu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie sesji Sparkowej\n",
    "Losujemy numer portu. Domyslnie port na ktorym Spark wystawia swoj interfejs graficzny jest 4040. zeby nie zajmowac sobie wzajmnie numerow portu - wykonamy losowanie.\n",
    "Nazwa aplikacji będzie udekorowana nazwą użytkownika który ją uruchamia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user_name = os.environ.get('USER')\n",
    "print(user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "ui_port = random.randint(4000,4999)\n",
    "print(ui_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".master('yarn-client') \\\n",
    ".config('spark.driver.memory','1g') \\\n",
    ".config('spark.executor.memory', '1g') \\\n",
    ".config('spark.ui.port',f'{ui_port}') \\\n",
    ".appName(f'app-{user_name}') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie danych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'/user/{user_name}/external/data/brca.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(path, format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charakterystyka danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type (df)                 # jaki jest typ danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()              # fizyczny plan wykonania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain(True)          # logiczny i fizyczny plan wykonania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions() # liczba partycji (bloków danych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()          # schemat danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()                # wymiary (liczba wierszy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)           # wymiary (liczba kolumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()      # pokaż podsumowanie danych w tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(\"Chromosome\").show() # pokaż podsumowanie danych w kolumnie "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odczyt danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()                    # pokaż 20 wierszy danych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, truncate=False) # pokaż 10 wierszy, nie skracaj danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Sample\").show()  # pokaż tylko kolumne sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Chromosome\", \"Start\", \"End\").show() # pokaż kolumny chromosome, start i end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chrom = df.select(df.Chromosome).distinct() # stworz nowy DF zawierajacy tylko unikalne wartosci chromosomow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chrom.count()                     # policz wiersze w nowym DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"Chromosome > 21\").show() # pokaz dane spelniajace warunek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"Chromosome > 21\").explain()  # pokaz plan wykonania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"Chromosome > 21 and Segment_Mean > 0\").show() # pokaz dane spelniajace warunki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grupowanie i funkcje agregujące"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.groupBy(\"Chromosome\").count().show()   # dokonaj grupowania po chromosomie i policz rekordy w grupie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"Chromosome\").avg(\"Segment_Mean\").show() # dokonaj grupowania po chromosomie i policz srednia wartosc segment_mean w grupie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pogrupuj dane wzgledem probki i chromosomu, policz rekordy w grupie\n",
    "df.groupBy(\"Sample\",\"Chromosome\").count().orderBy(asc(\"Sample\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolumny wyliczane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn (\"Length\", col(\"End\") - col(\"Start\")).show() # dodaj kolumne dlugosc jako end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn (\"Material\", lit(\"DNA\")).show()     # dodaj kolumne o stalej wartosci 'DNA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn (\"Chromosome2\", concat(lit(\"chr\"), col(\"Chromosome\"))).show() # dodaj kolumne z konkatencja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Chromosome\").show()  # usun kolumne Chromosome. \n",
    "# Czy DF została pozbawiona kolumny na trwale?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **** Zadanie 3 *****\n",
    "\n",
    "a) Ile jest unikalnych danych próbek w tym zbiorze danych?\n",
    "\n",
    "b) Pokaż nazwę próbki, numer chromosomu, początek i koniec segmentu dla segmentów występujących na chromosomie 21,22,23 i mających startową pozycję większą niż 10000000. \n",
    "\n",
    "c) Ile występuje wierszy dla każdej z próbki? Pokaż wynik z pełną nazwą próbki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przeniesienie pliku na GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp ~/data/brca.txt gs://bucket-$USER  # upload obiektu do kubełka"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "notebook_test": {
   "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
   "user": "ds-lab-testuser1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
