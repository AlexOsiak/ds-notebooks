{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user_name = os.environ.get('USER')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config('spark.driver.memory','1g') \\\n",
    ".config('spark.executor.memory', '2g') \\\n",
    ".getOrCreate()\n",
    "\n",
    "gs_path = f'gs://bucket-{user_name}/survey/2020/survey_results_public.csv'\n",
    "db_name = user_name.replace('-','_')\n",
    "spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
    "spark.sql(f'CREATE DATABASE {db_name}')\n",
    "spark.sql(f'USE {db_name}')\n",
    "table_name = \"survey_2020\" \n",
    "\n",
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true, NULLVALUE \"NA\") \\\n",
    "          LOCATION \"{gs_path}\"')\n",
    "\n",
    "# Przygotowanie danych do analizy\n",
    "\n",
    "spark_df= spark.sql(f'SELECT *, CAST((convertedComp > 60000) AS STRING) AS compAboveAvg \\\n",
    "                    FROM {table_name} WHERE convertedComp IS NOT NULL ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "y = 'compAboveAvg'      # chcemy przewidziec compAboveAvg\n",
    "feature_columns = ['OpSys', 'EdLevel', 'MainBranch' , 'Country', 'JobSeek', 'YearsCode']\n",
    "\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='strindexed_' + c).setHandleInvalid(\"keep\") for c in feature_columns]\n",
    "stringindexer_stages += [StringIndexer(inputCol=y, outputCol='label').setHandleInvalid(\"keep\")]\n",
    "\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol='strindexed_' + c, outputCol='onehot_' + c) for c in feature_columns]\n",
    "\n",
    "# Polaczenie wszystkich kolumn predykcyjnych do jednej (features) ASEMBLACJA\n",
    "extracted_columns = ['onehot_' + c for c in feature_columns]\n",
    "vectorassembler_stage = VectorAssembler(inputCols=extracted_columns, outputCol='features') \n",
    "\n",
    "# Polaczenie wszystkich krokow przygotowania danych w jednym potoku przetwarzania\n",
    "final_columns = [y] + feature_columns + extracted_columns + ['features', 'label']\n",
    "\n",
    "transformed_df = Pipeline(stages=stringindexer_stages + \\\n",
    "                          onehotencoder_stages + \\\n",
    "                          [vectorassembler_stage]).fit(spark_df).transform(spark_df).select(final_columns)\n",
    "training, test = transformed_df.randomSplit([0.8, 0.2], seed=1234) # Podzial na zbior treningowy/testowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRZEWO DECYZYJNE - bez parametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')\n",
    "dt_model = Pipeline(stages=[dt]).fit(training)\n",
    "pred_dt = dt_model.transform(test)\n",
    "label_and_pred = pred_dt.select('label', 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ewaluacje\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "evaluator_auroc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "evaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_f = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MLFLOW_TRACKING_URI=http://localhost:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "mlflow.set_experiment(experiment_name=\"Classifier\")\n",
    "experiment = mlflow.get_experiment_by_name('Classifier')\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"dt_model\"):\n",
    "  \n",
    "    mlflow.log_param(\"depth\", dt.getMaxDepth())\n",
    "\n",
    "    test_metric_auroc = evaluator_auroc.evaluate(dt_model.transform(test))\n",
    "    test_metric_acc = evaluator_acc.evaluate(dt_model.transform(test))\n",
    "    test_metric_recall = evaluator_recall.evaluate(dt_model.transform(test))\n",
    "    test_metric_prec = evaluator_prec.evaluate(dt_model.transform(test))\n",
    "    test_metric_f = evaluator_f.evaluate(dt_model.transform(test))\n",
    "\n",
    "    mlflow.log_metric(evaluator_auroc.getMetricName(), test_metric_auroc) \n",
    "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc) \n",
    "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall) \n",
    "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)     \n",
    "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f) \n",
    "  \n",
    "    mlflow.spark.log_model(spark_model=dt_model, artifact_path='classifier') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drzewo decyzyjne - walidacja krzyżowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(dt.maxDepth, [2,3,4,5,6]).\\\n",
    "    build()\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=param_grid, evaluator=evaluator_auroc, numFolds=4)\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"best_model\"):\n",
    "    cv_model = cv.fit(training)\n",
    "  \n",
    "    mlflow.log_param(\"depth\", cv_model.bestModel.depth)\n",
    "\n",
    "    test_metric_auroc = evaluator_auroc.evaluate(cv_model.bestModel.transform(test))\n",
    "    test_metric_acc = evaluator_acc.evaluate(cv_model.bestModel.transform(test))\n",
    "    test_metric_recall = evaluator_recall.evaluate(cv_model.bestModel.transform(test))\n",
    "    test_metric_prec = evaluator_prec.evaluate(cv_model.bestModel.transform(test))\n",
    "    test_metric_f = evaluator_f.evaluate(cv_model.bestModel.transform(test))\n",
    "\n",
    "    mlflow.log_metric(evaluator_auroc.getMetricName(), test_metric_auroc) \n",
    "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc) \n",
    "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall) \n",
    "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)     \n",
    "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f) \n",
    "  \n",
    "    mlflow.spark.log_model(spark_model=cv_model.bestModel, artifact_path='classifier') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "gbt_model = gbt.fit(training)\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"gbt_model\"):\n",
    "  \n",
    "    mlflow.log_param(\"depth\", gbt.getMaxDepth())\n",
    "\n",
    "    test_metric_auroc = evaluator_auroc.evaluate(gbt_model.transform(test))\n",
    "    test_metric_acc = evaluator_acc.evaluate(gbt_model.transform(test))\n",
    "    test_metric_recall = evaluator_recall.evaluate(gbt_model.transform(test))\n",
    "    test_metric_prec = evaluator_prec.evaluate(gbt_model.transform(test))\n",
    "    test_metric_f = evaluator_f.evaluate(gbt_model.transform(test))\n",
    "\n",
    "    mlflow.log_metric(evaluator_auroc.getMetricName(), test_metric_auroc) \n",
    "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc) \n",
    "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall) \n",
    "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)     \n",
    "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f) \n",
    "  \n",
    "    mlflow.spark.log_model(spark_model=gbt_model, artifact_path='classifier') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "notebook_test": {
   "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
   "user": "ds-lab-testuser1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
