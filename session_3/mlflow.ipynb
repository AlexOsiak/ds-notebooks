{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zajęcia 3\n",
    "\n",
    "## Zarządzanie i wdrażanie modeli ML\n",
    "Platform ML Flow (https://www.mlflow.org/docs/latest/index.html) umożliwia całościowe zarządzanie cyklem życia modeli.\n",
    "* logowanie eksperymentów, wartości parametrów modeli i osiąganych przez nie wyników\n",
    "* serializowanie modeli (na potrzeby współdzielenia modelu, przeniesienia na inne środowisko lub serwowania)\n",
    "* wersjonowanie modelu, adnotowanie i przechowywanie w Rejestrze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie danych do analizy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user_name = os.environ.get('USER')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config('spark.driver.memory','1g') \\\n",
    ".config('spark.executor.memory', '2g') \\\n",
    ".getOrCreate()\n",
    "\n",
    "gs_path = f'gs://bdg-lab-{user_name}/survey/2020/survey_results_public.csv'\n",
    "db_name = user_name.replace('-','_')\n",
    "spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
    "spark.sql(f'CREATE DATABASE {db_name}')\n",
    "spark.sql(f'USE {db_name}')\n",
    "table_name = \"survey_2020\" \n",
    "\n",
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true, NULLVALUE \"NA\") \\\n",
    "          LOCATION \"{gs_path}\"')\n",
    "\n",
    "spark_df= spark.sql(f'SELECT *, CAST((convertedComp > 60000) AS STRING) AS compAboveAvg \\\n",
    "                    FROM {table_name} WHERE convertedComp IS NOT NULL ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformacja danych do wektora cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "y = 'compAboveAvg' \n",
    "feature_columns = ['OpSys', 'EdLevel', 'MainBranch' , 'Country', 'JobSeek', 'YearsCode']\n",
    "\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='strindexed_' + c).setHandleInvalid(\"keep\") for c in feature_columns]\n",
    "stringindexer_stages += [StringIndexer(inputCol=y, outputCol='label').setHandleInvalid(\"keep\")]\n",
    "\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol='strindexed_' + c, outputCol='onehot_' + c) for c in feature_columns]\n",
    "extracted_columns = ['onehot_' + c for c in feature_columns]\n",
    "vectorassembler_stage = VectorAssembler(inputCols=extracted_columns, outputCol='features') \n",
    "\n",
    "final_columns = [y] + feature_columns + extracted_columns + ['features', 'label']\n",
    "\n",
    "transformed_df = Pipeline(stages=stringindexer_stages + \\\n",
    "                          onehotencoder_stages + \\\n",
    "                          [vectorassembler_stage]).fit(spark_df).transform(spark_df).select(final_columns)\n",
    "training, test = transformed_df.randomSplit([0.8, 0.2], seed=1234) # Podzial na zbior treningowy/testowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ML Flow: Definicja eksperymentu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "ename = f\"classifier_{user_name}\"\n",
    "mlflow.set_experiment(experiment_name=ename)\n",
    "experiment = mlflow.get_experiment_by_name(ename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicja metryk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "evaluator_auroc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "evaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_f = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Flow: logowanie eksperymentu z drzewem decyzyjnyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')\n",
    "dt_model = Pipeline(stages=[dt]).fit(training)\n",
    "pred_dt = dt_model.transform(test)\n",
    "label_and_pred = pred_dt.select('label', 'prediction')\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"dt_model\"):\n",
    "  \n",
    "    mlflow.log_param(\"depth\", dt.getMaxDepth())\n",
    "\n",
    "    test_metric_auroc = evaluator_auroc.evaluate(dt_model.transform(test))\n",
    "    test_metric_acc = evaluator_acc.evaluate(dt_model.transform(test))\n",
    "    test_metric_recall = evaluator_recall.evaluate(dt_model.transform(test))\n",
    "    test_metric_prec = evaluator_prec.evaluate(dt_model.transform(test))\n",
    "    test_metric_f = evaluator_f.evaluate(dt_model.transform(test))\n",
    "\n",
    "    mlflow.log_metric(evaluator_auroc.getMetricName(), test_metric_auroc) \n",
    "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc) \n",
    "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall) \n",
    "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)     \n",
    "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f) \n",
    "  \n",
    "    mlflow.spark.log_model(spark_model=dt_model, artifact_path='dt_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Flow: logowanie eksperymentu drzewa decyzyjnego z walidacją krzyżową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(dt.maxDepth, [2,3,4,5,6]).\\\n",
    "    build()\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=param_grid, evaluator=evaluator_auroc, numFolds=4)\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"best_model\"):\n",
    "    cv_model = cv.fit(training)\n",
    "  \n",
    "    mlflow.log_param(\"depth\", cv_model.bestModel.depth)\n",
    "\n",
    "    test_metric_auroc = evaluator_auroc.evaluate(cv_model.bestModel.transform(test))\n",
    "    test_metric_acc = evaluator_acc.evaluate(cv_model.bestModel.transform(test))\n",
    "    test_metric_recall = evaluator_recall.evaluate(cv_model.bestModel.transform(test))\n",
    "    test_metric_prec = evaluator_prec.evaluate(cv_model.bestModel.transform(test))\n",
    "    test_metric_f = evaluator_f.evaluate(cv_model.bestModel.transform(test))\n",
    "\n",
    "    mlflow.log_metric(evaluator_auroc.getMetricName(), test_metric_auroc) \n",
    "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc) \n",
    "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall) \n",
    "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)     \n",
    "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f) \n",
    "  \n",
    "    mlflow.spark.log_model(spark_model=cv_model.bestModel, artifact_path='best_classifier') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Flow: logowanie eksperymentu z modelem GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "gbt_model = gbt.fit(training)\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"gbt_model\"):\n",
    "  \n",
    "    mlflow.log_param(\"depth\", gbt.getMaxDepth())\n",
    "\n",
    "    test_metric_auroc = evaluator_auroc.evaluate(gbt_model.transform(test))\n",
    "    test_metric_acc = evaluator_acc.evaluate(gbt_model.transform(test))\n",
    "    test_metric_recall = evaluator_recall.evaluate(gbt_model.transform(test))\n",
    "    test_metric_prec = evaluator_prec.evaluate(gbt_model.transform(test))\n",
    "    test_metric_f = evaluator_f.evaluate(gbt_model.transform(test))\n",
    "\n",
    "    mlflow.log_metric(evaluator_auroc.getMetricName(), test_metric_auroc) \n",
    "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc) \n",
    "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall) \n",
    "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)     \n",
    "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f) \n",
    "  \n",
    "    mlflow.spark.log_model(spark_model=gbt_model, artifact_path='gbt_classifier') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Flow: serwowanie modelu\n",
    "\n",
    "Zeby zapewnić prosty interfejs do klasyfikatora, zapiszemy model wraz z krokami wstępnego przetwarzania (przekształcenie danych wejściowych w wektor cech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "raw_training, raw_test = spark_df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "full_classifier_name=\"full_gbt_classifier\"\n",
    "version=1\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"gbt_model_raw\"):\n",
    "    pipeline = Pipeline(stages=stringindexer_stages + \\\n",
    "                          onehotencoder_stages + \\\n",
    "                          [vectorassembler_stage] + [gbt] )\n",
    "    model = pipeline.fit(raw_training)\n",
    "    mlflow.spark.log_model(spark_model=model, artifact_path='gbt_classifier', registered_model_name=full_classifier_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasyfikacja jako funkcja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "registry_uri=f\"models:/{full_classifier_name}/{version}\"\n",
    "pyfunc_udf = mlflow.pyfunc.spark_udf(spark, model_uri=registry_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przykładowe wywołanie funkcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = raw_test.limit(10)\\\n",
    "    .withColumn(\"prediction\", pyfunc_udf(struct('OpSys', 'EdLevel', 'MainBranch' , 'Country', 'JobSeek', 'YearsCode'))) \\\n",
    "    .select ('OpSys', 'EdLevel', 'MainBranch' , 'Country', 'JobSeek', 'YearsCode', 'prediction')\n",
    "predicted_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasyfikacja jako serwis REST\n",
    "\n",
    "Sekwencja komend do uruchomienia w terminalu\n",
    "\n",
    "```bash\n",
    "unset PYSPARK_SUBMIT_ARGS\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "conda activate $HOME/venv/$JUPYTER_KERNEL_NAME\n",
    "mlflow models serve -m models:/full_gbt_classifier/1 -p 9090  --no-conda\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przykładowe wywołanie serwisu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "url = \"http://localhost:9090/invocations\"\n",
    "headers = {'Content-Type': 'application/json; format=pandas-split'}\n",
    "\n",
    "input_data = '{\"columns\":[\"OpSys\", \"EdLevel\", \"MainBranch\" , \"Country\", \"JobSeek\", \"YearsCode\"], \\\n",
    "    \"data\":[[ \\\n",
    "    \"MacOS\",\\\n",
    "    \"Master’s degree (M.A., M.S., M.Eng., MBA, etc.)\",\\\n",
    "    \"I am a developer by profession\",\"United Kingdom\",\\\n",
    "    \"I am not interested in new job opportunities\",\\\n",
    "    \"10\"]]}'\\\n",
    "    .encode('utf-8')\n",
    "\n",
    "r = requests.post(url, data=input_data, headers=headers) \n",
    "r.text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "notebook_test": {
   "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
   "user": "ds-lab-testuser1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
