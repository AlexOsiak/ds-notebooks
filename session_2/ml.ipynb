{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe problemy związane ze skalowalnością zadań ML:\n",
    "\n",
    "Ograniczenie CPU: Dane miesza sie w pamieci RAM, ale proces uczenia trwa za dlugo. Np. W przypadku koniecznosci sprawdzenia wielu kombinacji parametrow modelu, wielu modeli, itd. \n",
    "\n",
    "\n",
    "Ograniczenia pamieci: Dane sa na tyle duze ze nie mieszcza sie w pamieci RAM.\n",
    "\n",
    "\n",
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ml-Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ml-PipelineModel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potok przetwarzania ML\n",
    "\n",
    "* <b>DataFrame</b>: ten interfejs API ML używa DataFrame ze Spark SQL jako zestawu danych ML, który może przechowywać różne typy danych. Na przykład DataFrame może mieć różne kolumny przechowujące tekst, wektory cech, prawdziwe etykiety i prognozy.\n",
    "\n",
    "\n",
    "* <b>Transformer</b>: Transformator to algorytm, który może przekształcić jedną ramkę danych w inną ramkę danych. Na przykład model ML to transformator, który przekształca ramkę danych z funkcjami w ramkę danych z prognozami.\n",
    "\n",
    "\n",
    "* <b>Estimator</b>: Estimator to algorytm, który można dopasować do DataFrame w celu wytworzenia transformatora. Np. Algorytm uczenia się jest estymatorem, który trenuje na DataFrame i tworzy model.\n",
    "\n",
    "\n",
    "* <b>Pipeline</b>: Rurociąg łączy wiele transformatorów i estymatorów razem, aby określić przepływ pracy ML.\n",
    "\n",
    "\n",
    "* <b>Parametr</b>: Wszystkie transformatory i estymatory mają teraz wspólny interfejs API do określania parametrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user_name = os.environ.get('USER')\n",
    "print(user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config('spark.driver.memory','1g') \\\n",
    ".config('spark.executor.memory', '2g') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_path = f'gs://bdg-lab-{user_name}/survey/2020/survey_results_public.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = user_name.replace('-','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
    "spark.sql(f'CREATE DATABASE {db_name}')\n",
    "spark.sql(f'USE {db_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"survey_2020\"            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true, NULLVALUE \"NA\") \\\n",
    "          LOCATION \"{gs_path}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'describe {table_name}').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie danych do analizy\n",
    "\n",
    "W ramach zadania chcemy stworzyc klasyfikator, ktory bedzie przewidywac czy respondent zarabia wiecej niz 60000 USD rocznie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df= spark.sql(f'SELECT *, CAST((convertedComp > 60000) AS STRING) AS compAboveAvg \\\n",
    "                    FROM {table_name} where convertedComp IS NOT NULL ')\n",
    "spark_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dążymy do tego, żeby przygotować jeden wektor cech oraz jedną kolumnę z oznaczeniami. \n",
    "\n",
    "Pierwszy krok: feature extraction: kodujemy kolumny tekstowe na numeryczne, kodujemy wartosci liczbowe na reprezentacje onehotencoder. Nastepnie dokonujemy asemblacji do jednego wektora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "# chcemy przewidziec compAboveAvg\n",
    "y = 'compAboveAvg'\n",
    "# na podstawie:\n",
    "feature_columns = ['OpSys', 'EdLevel', 'MainBranch' , 'Country', 'JobSeek', 'YearsCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zaczynamy od transformatora StringIndexer, zamieniajacego wartosci 'string' na liczbe\n",
    "\n",
    "##### najpierw pokazujemy prosta petle z FOR, a potem zrefactorujmy do list comprehension\n",
    "\n",
    "# dla cech, ktore zostana wykorzystane do predykcji\n",
    "\n",
    "stringindexer_stages_1 = []\n",
    "for c in feature_columns:\n",
    "    stringindexer_stages_1.append (StringIndexer(inputCol=c, outputCol='strindexed_' + c).setHandleInvalid(\"keep\"))\n",
    "\n",
    "\n",
    "# i dla zmiennej objasnianej\n",
    "stringindexer_stages_1.append(StringIndexer(inputCol=y, outputCol='label').setHandleInvalid(\"keep\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactoring do list comprehension\n",
    "\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='strindexed_' + c).setHandleInvalid(\"keep\") for c in feature_columns]\n",
    "\n",
    "# i dla zmiennej objasnianej\n",
    "stringindexer_stages += [StringIndexer(inputCol=y, outputCol='label').setHandleInvalid(\"keep\")]\n",
    "stringindexer_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Po wykonaniu takiej transformacji do DF zostaje dodane  7 nowych kolumn z prefixem \"strindexed_\"\n",
    "Pipeline(stages=stringindexer_stages).fit(spark_df).transform(spark_df).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder_stages = [OneHotEncoder(inputCol='strindexed_' + c, outputCol='onehot_' + c) for c in feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozbudowujemy pipeline..\n",
    "#Po wykonaniu takiej transformacji do DF zostaje dodane  6 nowych kolumn z prefixem \"onehot_\". SparseV\n",
    "pa = Pipeline(stages=stringindexer_stages + onehotencoder_stages).fit(spark_df).transform(spark_df).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nowe kolumny zawieraja wartosci typu SparseVector zawierajacy mape bitowa.\n",
    "pa['onehot_OpSys'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polaczenie wszystkich kolumn predykcyjnych do jednej (features) ASEMBLACJA\n",
    "extracted_columns = ['onehot_' + c for c in feature_columns]\n",
    "vectorassembler_stage = VectorAssembler(inputCols=extracted_columns, outputCol='features') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polaczenie wszystkich krokow przygotowania danych w jednym potoku przetwarzania\n",
    "final_columns = [y] + feature_columns + extracted_columns + ['features', 'label']\n",
    "\n",
    "final_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = Pipeline(stages=stringindexer_stages + \\\n",
    "                          onehotencoder_stages + \\\n",
    "                          [vectorassembler_stage]).fit(spark_df).transform(spark_df).select(final_columns)\n",
    "\n",
    "transformed_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podzial na zbior treningowy/testowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = transformed_df.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uczenie modelu - model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na poczatek wybierzemy drzewo decyzyjne. Nie musimy podawac zadnych parametrow\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = Pipeline(stages=[dt]).fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model.stages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predykcja - model.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_simple = simple_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_columns = final_columns + ['prediction', 'rawPrediction', 'probability']\n",
    "pred_simple.limit(5).select(show_columns).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewaluacje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_and_pred = pred_simple.select('label', 'prediction')\n",
    "label_and_pred.groupBy('label', 'prediction').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ewaluator \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_simple = evaluator.evaluate(pred_simple)\n",
    "auroc_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_m = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_m.evaluate(pred_simple)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dodanie hiperparametrów "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jake wartosci hiperparametru maxDepth maja byc przetwstowane\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(dt.maxDepth, [2,3,4,5,6]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walidacja krzyrzowa wykonwyana w celu optymalizaji hiperparametrow\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budowa modelu na podstawie danych treningowych\n",
    "cv_model = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predykcja z nowym modelem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jak wyglada predykcja na zbiorze danych treninigowych?\n",
    "pred_cv = cv_model.transform(test)\n",
    "show_columns = final_columns + ['prediction', 'rawPrediction', 'probability']\n",
    "pred_cv.limit(5).select(show_columns).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "label_and_pred = pred_cv.select('label', 'prediction')\n",
    "label_and_pred.groupBy('label', 'prediction').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_cv = evaluator.evaluate(pred_cv)\n",
    "auroc_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_cv = evaluator_m.evaluate(pred_cv)\n",
    "acc_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja za pomca Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "model = gbt.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(model.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadania:\n",
    "\n",
    "* Czy mozna jeszcze poprawic jakosc predykcji: \n",
    "    * a) dodajac cechy\n",
    "    * b) zmieniajac model\n",
    "    * c) lepiej dobierajac parametry modelu ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kod w R\n",
    "#library(data.table)\n",
    "#srv <- fread(\"survey_results_public.csv\")\n",
    "#srv$OpSys2 <- srv$OpSys == \"Windows\"\n",
    "#library(rpart)\n",
    "#srv$CompAboveAvg <- CompAboveAvg$ConvertedComp > 60e3\n",
    "#dt_fit = rpart(CompAboveAvg ~ Age + EdLevel + JobSeek + OpSys + YearsCode , data = srv, method = 'class')\n",
    "#pred_y = predict(dt_fit, type = 'class')\n",
    "#table(predict(dt_fit, srv[,c(\"Age\" , \"EdLevel\", \"JobSeek\", \"OpSys\", \"YearsCode\")], type = \"class\"), srv$CompAboveAvg)\n",
    "#srv(cor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "notebook_test": {
   "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
   "user": "ds-lab-testuser1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
